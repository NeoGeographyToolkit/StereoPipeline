#!/usr/bin/env python
# __BEGIN_LICENSE__
#  Copyright (c) 2009-2013, United States Government as represented by the
#  Administrator of the National Aeronautics and Space Administration. All
#  rights reserved.
#
#  The NGT platform is licensed under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance with the
#  License. You may obtain a copy of the License at
#  http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
# __END_LICENSE__

# for debugging:
import sys, optparse, subprocess, re, os, math, time, datetime
from osgeo import gdal, gdalconst
#import os.path as P
import numpy as np
import scipy as sp
import scipy.ndimage.filters as sf
import scipy.stats as ss
import scipy.spatial as sp
from scipy.ndimage import convolve, binary_dilation, convolve1d
from scipy.sparse import coo_matrix
from scipy.interpolate import griddata
from multiprocessing import Pool, cpu_count
from optparse import OptionParser

def die(msg, code=-1):
    print >>sys.stderr, msg
    sys.exit(code)

def prevpow2(i):
    """
    Find largest 2^n that is <= given number.
    """
    n = 1
    while 2*n <= i: n *= 2
    return n

class im_subset:
    def __init__(self, c0, r0, Nc, Nr, source, user_nodata, pad_val=0, Bands=(1,2,3)):
        self.source=source
        self.c0=c0
        self.r0=r0
        self.Nc=Nc
        self.Nr=Nr
        self.z=[]
        # if the level is zero, this is a copy of a file, if it's >0, it's a copy of a copy of a file
        self.level=0
        self.Bands=Bands
        self.pad_val=pad_val
        # get the nodata value from the source
        if hasattr(self.source, 'level'):
            self.level=self.source.level+1
            if user_nodata is not None:
                self.noData = user_nodata
            else:
                self.noData=self.source.noData
        else:
            band=self.source.GetRasterBand(self.Bands[0])
            if user_nodata is not None:
                self.noData = user_nodata
            else:
                self.noData=band.GetNoDataValue()
            if self.noData is None:
                self.noData=0.0

    def setBounds(self, c0, r0, Nc, Nr, update=0):
        self.c0=np.int(c0)
        self.r0=np.int(r0)
        self.Nc=np.int(Nc)
        self.Nr=np.int(Nr)
        if update > 0:
            self.copySubsetFrom(pad_val=self.pad_val)

    def copySubsetFrom(self, pad_val=0):
        if hasattr(self.source, 'level'):  # copy data from another subset
            self.z = np.zeros((self.source.z.shape[0], self.Nr, self.Nc), self.source.z.dtype) + pad_val
            (sr0, sr1, dr0, dr1, vr)=match_range(self.source.r0, self.source.Nr, self.r0, self.Nr)
            (sc0, sc1, dc0, dc1, vc)=match_range(self.source.c0, self.source.Nc, self.c0, self.Nc)
            if (vr & vc):
                self.z[:, dr0:dr1, dc0:dc1]=self.source.z[:,sr0:sr1, sc0:sc1]
            self.level=self.source.level+1
        else:  # read data from a file
            band=self.source.GetRasterBand(self.Bands[0])
            src_NB=self.source.RasterCount
            dt=gdal.GetDataTypeName(band.DataType)
            self.z=np.zeros((src_NB, self.Nr, self.Nc), dt)+pad_val
            (sr0, sr1, dr0, dr1, vr)=match_range(0, band.YSize, self.r0, self.Nr)
            (sc0, sc1, dc0, dc1, vc)=match_range(0, band.XSize, self.c0, self.Nc)
            if (vr & vc):
                a=self.source.ReadAsArray(int(sc0),  int(sr0), int(sc1-sc0), int(sr1-sr0))
                self.z[:, dr0:dr1, dc0:dc1]=a
            self.level=0

    def trim_from_edges(self, x_trim, y_trim):
        self.z=self.z[:, y_trim[0]:-y_trim[1], x_trim[0]:-x_trim[1]]
        self.r0=self.r0+y_trim[0]
        self.c0=self.c0+x_trim[0]
        self.Nr=self.Nr-np.sum(y_trim)
        self.Nc=self.Nc-np.sum(x_trim)

    def writeSubsetTo(self, bands, target):
        if hasattr(target, 'level') and target.level > 0:
            (sr0, sr1, dr0, dr1, vr)=match_range(target.source.r0, target.source.Nr, self.r0, self.Nr)
            (sc0, sc1, dc0, dc1, vc)=match_range(target.source.c0, target.source.Nc, self.c0, self.Nc)
            if (vr & vc):
                for b in bands:
                    target.source.z[b,sr0:sr1, sc0:sc1]=self.z[b, dr0:dr1, dc0:dc1]
        else:
            band=target.source.GetRasterBand(1)
            (sr0, sr1, dr0, dr1, vr)=match_range(0, band.YSize, self.r0, self.Nr)
            (sc0, sc1, dc0, dc1, vc)=match_range(0, band.XSize, self.c0, self.Nc)
            if (vr & vc):
                for bb in (bands):
                    band=target.source.GetRasterBand(int(bb))
                    band.WriteArray( self.z[bb-1, dr0:dr1, dc0:dc1], int(sc0), int(sr0))

def match_range(s0, ns, d0, nd):
    i0 = max(s0, d0)
    i1 = min(s0+ns, d0+nd)
    si0=max(0, i0-s0)
    si1=min(ns, i1-s0)
    di0=max(0, i0-d0)
    di1=min(nd,i1-d0)
    any_valid=(di1>di0) & (si1 > si0)
    return (si0, si1, di0, di1, any_valid)

# Try and use the faster Fourier transform functions from the anfft
# module if available. Otherwise use the normal scipy fftpack ones
# instead (~2-3x slower!).
try:
    import anfft as _anfft
    # measure == True for self-optimisation of repeat Fourier transforms of
    # similarly-shaped arrays
    def fftn(A,shape=None):
            if shape != None:
                    A = _checkffttype(A)
                    A = procrustes(A,target=shape,side='after',padval=0)
            return _anfft.fftn(A,measure=True)
    def ifftn(B,shape=None):
            if shape != None:
                    B = _checkffttype(B)
                    B = procrustes(B,target=shape,side='after',padval=0)
            return _anfft.ifftn(B,measure=True)
    def _checkffttype(C):
            # make sure input arrays are typed correctly for FFTW
            if C.dtype == 'complex256':
                    # the only incompatible complex type --> complex64
                    C = np.complex128(C)
            elif C.dtype not in ['float32','float64','complex64','complex128']:
                    # any other incompatible type --> float64
                    C = np.float64(C)
            return C

except ImportError:
    print "Module 'anfft' (FFTW Python bindings) could not be imported.\n"\
          "To install it, try running 'easy_install anfft' from the terminal.\n"\
          "Falling back on the slower 'fftpack' module for ND Fourier transforms."
    from scipy.fftpack import fftn, ifftn

class TemplateMatch(object):
    """
    N-dimensional template search by normalized cross-correlation or sum of
    squared differences.

    Arguments:
    ------------------------
        template    The template to search for

    """
    def __init__(self,template):

        self.template = template

    def __call__(self,a):

        if a.ndim != self.template.ndim:
            raise Exception('Input array must have the same number '\
                    'of dimensions as the template (%i)'
                    %self.template.ndim)

        C=norm_xcorr(self.template,a,trim=True)
        return  C

def norm_xcorr(t,a,trim=True):
    """
    Fast normalized cross-correlation for n-dimensional arrays

    Inputs:
    ----------------
        t   The template. Must have at least 2 elements, which
            cannot all be equal.

        a   The search space. Its dimensionality must match that of
            the template.

        trim    If True (default), the output array is trimmed down to
            the size of the search space. Otherwise, its size will
            be (f.shape[dd] + t.shape[dd] -1) for dimension dd.

    Output:
    ----------------
        nxcorr  An array of cross-correlation coefficients, which may
            vary from -1.0 to 1.0.

    Wherever the search space has zero  variance under the template,
    normalized  cross-correlation is undefined. In such regions, the
    correlation coefficients are set to zero.

    References:
        Hermosillo et al 2002: Variational Methods for Multimodal Image
        Matching, International Journal of Computer Vision 50(3),
        329-343, 2002
        <http://www.springerlink.com/content/u4007p8871w10645/>

        Lewis 1995: Fast Template Matching, Vision Interface,
        p.120-123, 1995
        <http://www.idiom.com/~zilla/Papers/nvisionInterface/nip.html>

        <http://en.wikipedia.org/wiki/Cross-correlation#Normalized_cross-correlation>

    Alistair Muldal
    Department of Pharmacology
    University of Oxford
    <alistair.muldal@pharm.ox.ac.uk>

    Sept 2012

    """

    if t.size < 2:
        raise Exception('Invalid template')
    if t.size > a.size:
        raise Exception('The input array must be smaller than the template')

    std_t,mean_t = np.std(t),np.mean(t)

    if std_t == 0:
        #raise Exception('The values of the template must not all be equal (they are equal to %f)' % mean_t)
        #warnings.warn('The values of the template are all the same, correlation failed')
        return None

    t = np.float64(t)
    a = np.float64(a)

    # output dimensions of xcorr need to match those of local_sum
    outdims = np.array([a.shape[dd]+t.shape[dd]-1 for dd in xrange(a.ndim)])
    # hack by BS 1/15/2013-- the direct method appears not to work

    af = fftn(a,shape=outdims)
    tf = fftn(ndflip(t),shape=outdims)
    # 'non-normalized' cross-correlation
    xcorr = np.real(ifftn(tf*af))

    # local linear and quadratic sums of input array in the region of the
    # template
    ls_a = local_sum(a,t.shape, 0)
    ls2_a = local_sum(a**2,t.shape, np.mean(a**2))

    # now we need to make sure xcorr is the same size as ls_a
    xcorr = procrustes(xcorr,ls_a.shape,side='both')

    # local standard deviation of the input array
    ls_diff = ls2_a - (ls_a**2)/t.size
    ls_diff = np.where(ls_diff < 0,0,ls_diff)
    sigma_a = np.sqrt(ls_diff)

    # standard deviation of the template
    sigma_t = np.sqrt(t.size-1.)*std_t

    # denominator: product of standard deviations
    denom = sigma_t*sigma_a

    # numerator: local mean corrected cross-correlation
    numer = (xcorr - ls_a*mean_t)

    # sigma_t cannot be zero, so wherever the denominator is zero, this must
    # be because sigma_a is zero (and therefore the normalized cross-
    # correlation is undefined), so set nxcorr to zero in these regions
    tol = np.sqrt(np.finfo(denom.dtype).eps)
    nxcorr=np.zeros(numer.shape)
    good=denom>tol
    nxcorr[good]=numer[good]/denom[good]
    #nxcorr = np.where(denom < tol,0,numer/denom)

    # if any of the coefficients are outside the range [-1 1], they will be
    # unstable to small variance in a or t, so set them to zero to reflect
    # the undefined 0/0 condition
    bad=nxcorr-1. >  np.sqrt(np.finfo(nxcorr.dtype).eps);
    nxcorr[bad]=0.

    if trim:
        nxcorr = procrustes(nxcorr,a.shape,side='both')
        sigma_a =procrustes(sigma_a,a.shape,side='both')
    return nxcorr

def local_sum(a,tshape, padval):
    """For each element in an n-dimensional input array, calculate
    the sum of the elements within a surrounding region the size of
    the template"""

    # zero-padding
    a = ndpad(a,tshape, padval)

    # difference between shifted copies of an array along a given dimension
    def shiftdiff(a,tshape,shiftdim):
        ind1 = [slice(None,None),]*a.ndim
        ind2 = [slice(None,None),]*a.ndim
        ind1[shiftdim] = slice(tshape[shiftdim],a.shape[shiftdim]-1)
        ind2[shiftdim] = slice(0,a.shape[shiftdim]-tshape[shiftdim]-1)
        return a[ind1] - a[ind2]

    # take the cumsum along each dimension and subtracting a shifted version
    # from itself. this reduces the number of computations to 2*N additions
    # and 2*N subtractions for an N-dimensional array, independent of its
    # size.
    #
    # See:
    # <http://www.idiom.com/~zilla/Papers/nvisionInterface/nip.html>
    for dd in xrange(a.ndim):
        a = np.cumsum(a,dd)
        a = shiftdiff(a,tshape,dd)
    return a


def ndpad(a,npad=None,padval=0):
    """
    Pads the edges of an n-dimensional input array with a constant value
    across all of its dimensions.

    Inputs:
    ----------------
        a   The array to pad

        npad*   The pad width. Can either be array-like, with one
            element per dimension, or a scalar, in which case the
            same pad width is applied to all dimensions.

        padval  The value to pad with. Must be a scalar (default is 0).

    Output:
    ----------------
        b   The padded array

    *If npad is not a whole number, padding will be applied so that the
    'left' edge of the output is padded less than the 'right', e.g.:

        a       == np.array([1,2,3,4,5,6])
        ndpad(a,1.5)    == np.array([0,1,2,3,4,5,6,0,0])

    In this case, the average pad width is equal to npad (but if npad was
    not a multiple of 0.5 this would not still hold). This is so that ndpad
    can be used to pad an array out to odd final dimensions.
    """

    if npad == None:
        npad = np.ones(a.ndim)
    elif np.isscalar(npad):
        npad = (npad,)*a.ndim
    elif len(npad) != a.ndim:
        raise Exception('Length of npad (%i) does not match the '\
                'dimensionality of the input array (%i)'
                %(len(npad),a.ndim))

    # initialise padded output
    padsize = [a.shape[dd]+2*npad[dd] for dd in xrange(a.ndim)]
    b = np.ones(padsize,a.dtype)*padval

    # construct an N-dimensional list of slice objects
    ind = [slice(np.floor(npad[dd]),a.shape[dd]+np.floor(npad[dd])) for dd in xrange(a.ndim)]

    # fill in the non-pad part of the array
    b[ind] = a
    return b

def procrustes(a,target,side='both',padval=0):
    """
    Forces an array to a target size by either padding it with a constant or
    truncating it

    Arguments:
        a   Input array of any type or shape
        target  Dimensions to pad/trim to, must be a list or tuple
    """

    try:
        if len(target) != a.ndim:
            raise TypeError('Target shape must have the same number of dimensions as the input')
    except TypeError:
        raise TypeError('Target must be array-like')

    try:
        b = np.ones(target,a.dtype)*padval
    except TypeError:
        raise TypeError('Pad value must be numeric')
    except ValueError:
        raise ValueError('Pad value must be scalar')

    aind = [slice(None,None)]*a.ndim
    bind = [slice(None,None)]*a.ndim

    # pad/trim comes after the array in each dimension
    if side == 'after':
        for dd in xrange(a.ndim):
            if a.shape[dd] > target[dd]:
                aind[dd] = slice(None,target[dd])
            elif a.shape[dd] < target[dd]:
                bind[dd] = slice(None,a.shape[dd])

    # pad/trim comes before the array in each dimension
    elif side == 'before':
        for dd in xrange(a.ndim):
            if a.shape[dd] > target[dd]:
                aind[dd] = slice(a.shape[dd]-target[dd],None)
            elif a.shape[dd] < target[dd]:
                bind[dd] = slice(target[dd]-a.shape[dd],None)

    # pad/trim both sides of the array in each dimension
    elif side == 'both':
        for dd in xrange(a.ndim):
            if a.shape[dd] > target[dd]:
                diff = (a.shape[dd]-target[dd])/2.
                aind[dd] = slice(np.floor(diff),a.shape[dd]-np.ceil(diff))
            elif a.shape[dd] < target[dd]:
                diff = (target[dd]-a.shape[dd])/2.
                bind[dd] = slice(np.floor(diff),target[dd]-np.ceil(diff))

    else:
        raise Exception('Invalid choice of pad type: %s' %side)

    b[bind] = a[aind]

    return b

def ndflip(a):
    """Inverts an n-dimensional array along each of its axes"""
    ind = (slice(None,None,-1),)*a.ndim
    return a[ind]

def log_filter(img, noData):
    """
    performs a separable Laplacian of Gaussian filter on the input image.
    Returns a copy of the original image, converted to float64
    """
    img1 = np.float64(img.copy())
    img1 = sf.laplace( img1, mode='constant')
    if np.sum(img<=noData) > 0 : #zero the filtered image at  data-nodata boundaries
        mask = np.float64(img.copy())
        mask[:] = 1.
        mask[img<=noData] = 0.0
        mask=sf.laplace(mask, mode='constant')
        img1[mask != 0.] = 0.   # set the borders to zero
    img1 = sf.gaussian_filter(img1, (1.4, 1.4), mode='constant')
    return img1

def run_one_block(t_pixels, KW, min_template_sigma, Xc, Yc,
                  s_nx, s_ny, dx0, dy0, T_img, S_img, noData):

    # Do template matching for a single block in the left image.
    # LOG filter the images
    T_filt=log_filter(T_img, noData)
    S_filt=log_filter(S_img, noData)
    std_T=np.std(T_filt)
    if min_template_sigma is not None:
        if std_T <= min_template_sigma:
            return(-2, Xc, Yc, -1, 0, 0)

    # run the feature match.
    TT=T_filt[KW:t_pixels+KW, KW:t_pixels+KW]
    SS=S_filt[KW:s_ny+KW, KW:s_nx+KW]
    # If the search image is large, do an initial search at 2x lower resolution
    if (SS.shape[0] > 32+TT.shape[0]) or (SS.shape[1] >32+TT.shape[1]):
        TT1=TT[1:-1:2, 1:-1:2]
        SS1=SS[1:-1:2, 1:-1:2]
        TM=TemplateMatch(TT1)
        result=TM(SS1)
        if result is None:
            #warnings.warn('run_one_block: TemplateMatch returned None at xc=%d, yc=%d' % (Xc, Yc))
            return(-3., Xc, Yc, 0., np.NaN, np.NaN)

        result=result[(t_pixels/4):(s_ny/2-t_pixels/4), (t_pixels/4):(s_nx/2-t_pixels/4)]
        ijC = 2*(np.array(np.unravel_index(np.argmax(result), result.shape))-[result.shape[0]/2., result.shape[1]/2.]).astype(int)

        t_xr=np.array(ijC[1]+[-t_pixels/2-16, t_pixels/2+16]+SS.shape[1]/2, dtype=np.int16)
        t_yr=np.array(ijC[0]+[-t_pixels/2-16, t_pixels/2+16]+SS.shape[0]/2, dtype=np.int16)
        if t_xr[0] >= 0 and t_yr[0] >= 0 and t_xr[1] <= SS.shape[1] and t_yr[1] <= SS.shape[0] :
            dx0=dx0+ijC[1]
            dy0=dy0+ijC[0]
            SS=SS[t_yr[0]:t_yr[1], t_xr[0]:t_xr[1]]

    TM=TemplateMatch(TT)
    result=TM(SS)

    # trim off edges of result
    result=result[(t_pixels/2):(SS.shape[0]-t_pixels/2),
                  (t_pixels/2):(SS.shape[1]-t_pixels/2)]

    ij = np.unravel_index(np.argmax(result), result.shape)
    ijC=ij-np.array([result.shape[0]/2, result.shape[1]/2])

    return (result[ij], Xc, Yc, std_T, ijC[1]+dx0, ijC[0]+dy0)

def run_blocks(param):

    # Run template matching for a set of blocks. This function is being
    # distributed across multiple processors.

    (Tfile, Sfile, processes, xgi, ygi, these, t_pixels, s_nxy_i,
     dxy0_i, XYc_i, min_template_sigma, user_nodata) = param
    matcher = fft_matcher(Tfile, Sfile, processes, user_nodata)

    KW=matcher.KW

    s_x_bounds=np.c_[XYc_i[:,0]+dxy0_i[:,0]-s_nxy_i[:,0]-KW-1000,
                     XYc_i[:,0]+dxy0_i[:,0]+s_nxy_i[:,0]+KW+1000]
    s_y_bounds=np.c_[XYc_i[:,1]+dxy0_i[:,1]-s_nxy_i[:,1]-KW-1000,
                     XYc_i[:,1]+dxy0_i[:,1]+s_nxy_i[:,1]+KW+1000]
    matcher.T_sub.setBounds(xgi-t_pixels, ygi-t_pixels, matcher.blocksize+2*t_pixels,
                            matcher.blocksize+2*t_pixels, update=1)
    matcher.S_sub.setBounds(s_x_bounds[these,0].min(),
                            s_y_bounds[these,0].min(),
                            s_x_bounds[these,1].max()-s_x_bounds[these,0].min(),
                            s_y_bounds[these,1].max()-s_y_bounds[these,0].min(),
                            update=1)
    these_ind = np.array(np.nonzero(these)).ravel()
    indices = []; c = []; x = []; y = []; sigma = []; dx = []; dy = [];

    # loop over the sub-blocks

    count=-1
    for Xc, Yc, s_nx, s_ny, dx0, dy0 in zip(XYc_i[these,0], XYc_i[these,1], s_nxy_i[these,0],
                                            s_nxy_i[these,1], dxy0_i[these,0],
                                            dxy0_i[these,1]):
        #print "S_buffer.Nodata=%f, T_buffer.noData"% (matcher.S_sub.noData, matcher.T_sub.noData)
        count=count+1
        t_xr=[Xc-(t_pixels/2-1)-KW, Xc+(t_pixels/2)+KW]
        t_yr=[Yc-(t_pixels/2-1)-KW, Xc+(t_pixels/2)+KW]
        s_xr=[Xc+dx0-(s_nx/2-1)-KW, Xc+(s_nx/2)+KW]
        s_yr=[Yc+dy0-(s_ny/2-1)-KW, Yc+(s_ny/2)+KW]

        # Read in the data for this block. Use the im_subset objects:
        # read nodata if we read past the image edges.

        # Read T
        T_buffer=im_subset(0, 0, matcher.T_band.XSize, matcher.S_band.YSize,
                           matcher.T_sub, user_nodata, pad_val=matcher.T_sub.noData)
        T_buffer.setBounds(t_xr[0]-KW, t_yr[0]-KW, t_pixels+2.*KW,
                           t_pixels+2.*KW, update=1)
        T_img=T_buffer.z[0,:,:]
        if np.mean(T_img<=T_buffer.noData)>.1: # bail if > 10% 0, flag with C=-2
            continue

        # Read S
        S_buffer=im_subset(0, 0, matcher.S_band.XSize, matcher.S_band.YSize,
                           matcher.S_sub, user_nodata, pad_val=matcher.S_sub.noData)
        S_buffer.setBounds(s_xr[0]-KW, s_yr[0]-KW, s_nx+2.*KW,
                           s_ny+2.*KW, update=1)
        S_img=S_buffer.z [0,:,:]
        if np.mean(S_img<=S_buffer.noData) > .25: # bail if > 25% 0
            continue
        out = run_one_block(t_pixels, KW, min_template_sigma, Xc, Yc, s_nx,
                            s_ny, dx0, dy0, T_img, S_img, T_buffer.noData)

        indices.append(these_ind[count])
        c.append(out[0])
        x.append(out[1])
        y.append(out[2])
        sigma.append(out[3])
        dx.append(out[4])
        dy.append(out[5])

    return(indices[:], c[:], x[:], y[:], sigma[:], dx[:], dy[:])

class fft_matcher(object):
    """
    class to perform fft matches on a pair of image files.  Uses the GDAL
    API for reads and writes, and the norm_xcorr package to do the matching
    Arguments:
        For initialization:
            Tfile  The template file-- small images are extracted from this file
                and correlated against sub-images of Sfile
            Sfile  The search file.

        For correlation:
            t_pixels : Size of the square template
            s_nxy_i  : [2, N], array-like  search windows in x and y
                        to use at each center (n_columns, n_rows)
            dxy0_i   : [2, N], array-like  initial offset estimates
                        (delta_col,  delta_row)
            XYc_i    : [2,N] pixel centers to search in the template image
                        (col, row)

        Outputs from correlation:
            xyC     :  Pixel centers in the template image
            dxy     :  pixel offsets needed to shift the template to match the search image
            C       :  Correlation value for the best match (0<C<1).
                        -1 indicates invalid search or template data
    """
    def __init__(self, Tfile, Sfile, processes, user_nodata):
        self.Tfile = Tfile
        self.Sfile = Sfile
        self.processes = processes
        self.T_ds=gdal.Open(Tfile, gdalconst.GA_ReadOnly)
        self.T_band=self.T_ds.GetRasterBand(1)
        self.S_ds=gdal.Open(Sfile, gdalconst.GA_ReadOnly)
        self.S_band=self.S_ds.GetRasterBand(1)
        self.Nx=self.T_band.XSize
        self.Ny=self.T_band.YSize
        self.KW=13  # pad the edges by this amount to avoid edge effects
        self.S_sub=im_subset(0, 0, self.S_band.XSize, self.S_band.YSize,
                         self.S_ds, user_nodata, pad_val=0, Bands=[1])
        self.T_sub=im_subset(0, 0, self.T_band.XSize, self.S_band.YSize,
                         self.T_ds, user_nodata, pad_val=0, Bands=[1])

        GT_S=self.S_sub.source.GetGeoTransform()
        self.GT_S=GT_S
        self.UL_S=np.array([GT_S[0], GT_S[3]])
        LL_S=np.array([GT_S[0], GT_S[3]+self.S_sub.Nr*GT_S[5]])
        UR_S=np.array([GT_S[0]+self.S_sub.Nc*GT_S[1], GT_S[3]])

        GT_T=self.T_sub.source.GetGeoTransform()
        self.GT_T=GT_T
        self.UL_T=np.array([GT_T[0], GT_T[3]])
        LL_T=np.array([GT_T[0], GT_T[3]+self.T_sub.Nr*GT_T[5]])
        UR_T=np.array([GT_T[0]+self.T_sub.Nc*GT_T[1], GT_T[3]])

        XR=[np.max([LL_S[0], LL_T[0]]), np.min([UR_S[0], UR_T[0]])]
        YR=[np.max([LL_S[1], LL_T[1]]), np.min([UR_S[1], UR_T[1]])]
        self.XR=XR
        self.YR=YR
        self.T_c0c1=np.array([max([0, (LL_T[0]-XR[0])/GT_T[1]]),
                              min(self.T_sub.Nc, (XR[1]-LL_T[0])/GT_T[1])]).astype(int)
        self.T_r0r1=np.array([max([0 , (YR[1]-UR_T[1])/GT_T[5]]),
                              min([self.T_sub.Nr, (LL_T[1]-YR[1])/GT_T[5]])]).astype(int)

        self.blocksize=8192/4
        self.user_nodata = user_nodata

    def __call__(self, t_pixels, s_nxy_i, dxy0_i, XYc_i, min_template_sigma):

        # loop over pixel centers

        self.C=np.zeros([XYc_i.shape[0], 1])-1
        self.sigma_template=(self.C).copy()
        self.dxy=np.zeros([XYc_i.shape[0], 2])
        self.xyC=np.zeros([XYc_i.shape[0], 2])

        xg0, yg0=np.meshgrid(np.arange(0, self.T_band.XSize, self.blocksize),
                             np.arange(0, self.T_band.YSize, self.blocksize))

        TaskParams = []
        for xgi, ygi in zip(xg0.ravel(), yg0.ravel()):
            these=np.logical_and(np.logical_and(XYc_i[:,0] > xgi,
                                                XYc_i[:,0] <= xgi+self.blocksize),
                                 np.logical_and(XYc_i[:,1] > ygi,
                                                XYc_i[:,1] <= ygi+self.blocksize))
            if ~np.any(these):
                continue
            param = (self.Tfile, self.Sfile, self.processes, xgi.copy(), ygi.copy(), these.copy(),
                     t_pixels, s_nxy_i.copy(), dxy0_i.copy(), XYc_i.copy(), min_template_sigma,
                     self.user_nodata)
            TaskParams.append(param)

        if self.processes > 0: # Run using multiple processes
            pool = Pool(processes=self.processes)
            Out = pool.map(run_blocks, TaskParams)
        else: # Run using single process (for debugging)
            Out=[run_blocks(TP) for TP in TaskParams]

        for out in Out:
            (indices, c, x, y, sigma, dx, dy) = out

            for i in range(len(indices)):
                ind = indices[i]
                self.C[ind] = c[i]
                self.xyC[ind] = [x[i], y[i]]
                self.sigma_template[ind] = sigma[i]
                self.dxy[ind,:] = [dx[i], dy[i]]

        return (self.xyC).copy(), (self.dxy).copy(), (self.C).copy(), (self.sigma_template).copy()

def make_pt_2_neighbors(tri):
    """
    make a dictionary of the neighbors of each point in triangulation
    tri
    """
    pt_dict=dict()
    for vlist in tri.vertices:
        for i in vlist:
            if not i in pt_dict:
                pt_dict[i]=list()
            for k in vlist:
                if k != i:
                    pt_dict[i].insert(0,k)
    for i in range(tri.points.shape[0]):
        pt_dict[i]=np.unique(pt_dict[i]).tolist()
    return pt_dict


def unique_rows(data):
    """
    return the indices for he unique rows of matrix (data)
    """
    udict = dict()
    for row in range(len(data)):
        row_data = tuple(data[row,:])
        if not row_data in udict:
            udict[row_data] =row
    uInd=udict.values()
    uRows=np.c_[udict.keys()]
    return uInd, uRows

def search_new_pts(xy, dxy, t_size, matcher, min_template_sigma=0., mask=None):
    """
    use an image matcher object to perform a template match between
    two images at the points defined by xy, using search windows that
    span the range in dxy
    """
    if mask is not None:
        x_geo=matcher.GT_T[0]+matcher.GT_T[1]*xy[:,0]
        y_geo=matcher.GT_T[3]+matcher.GT_T[5]*xy[:,1]
        good_pts=img_interpolate_linear(x_geo, y_geo, mask['Z'], mask['GT'])
        good_pts=(~np.isnan(good_pts)) & (good_pts > 0)
        if ~np.any(good_pts):
            return np.zeros(0), np.zeros(0), np.zeros(0), np.zeros(0)
        xy=xy[good_pts,:]
        dxy=dxy[good_pts,:]

    dxy0 = np.c_[dxy[:,0]+dxy[:,1], dxy[:,2]+dxy[:,3]]/2.
    dxyR = np.c_[dxy[:,1]-dxy[:,0], dxy[:,3]-dxy[:,2]]
    sw = np.array(2.**np.ceil(np.log2(np.maximum(dxyR+t_size, t_size+16))))
    xy_new, dxy_new, C_new, Pt_new= matcher(t_size, sw, dxy0, xy,
                                            min_template_sigma=min_template_sigma)
    xy_bad_mask=xy_new[C_new.ravel()==-2,:]
    good=C_new>0
    xy_new=xy_new[good.ravel(),:]
    dxy_new=dxy_new[good.ravel(),:]
    C_new=C_new[good.ravel()]
    Pt_new=Pt_new[good.ravel()]
    return xy_new, dxy_new, C_new, xy_bad_mask

def nbhd_range(pt_list, dx_mat, dy_mat, tri, pt_dict, max_dist=None, calc_min_slope=None):
    """
    for each point in pt_list, return the maximum and minimum offset
    for its neighbors.
    inputs:
        pt_list:  list of points to be searched
        dx_mat: sparse matrix with x disparity values for each pixel in the
                template image
        dy_may: ditto, but for y
        tri:    a triangulation.  th points field in this triangulation is
                used to get the x and y offsets for the point indices
        pt_dict:  a dictionary giving the neighbor point numbers for each
                  point in tri
    output:
        dx_range: an Nx4 martix.  columns 0 and 1 give the min and max x
        offsets around each point, columns 2 and 3 give the min and max y.
    """
    dxy_range=np.zeros([len(pt_list), 4])
    if calc_min_slope is not None:
        dxy_slope=np.zeros([len(pt_list), 2])
    for row, pt in enumerate(pt_list):
        neighbors=pt_dict[pt]
        nbhd_pts=tri.points[neighbors,:]
        this_pt=tri.points[pt,:]
        if max_dist is not None:
            dist2=(nbhd_pts[:,1]-this_pt[1])**2 + (nbhd_pts[:,0]-this_pt[0])**2
            nbhd_pts=nbhd_pts[dist2 < max_dist**2,:]
        if nbhd_pts is None or min(nbhd_pts.shape) < 1:
            continue
        dx_vals=np.append(np.array(dx_mat[nbhd_pts[:,1], nbhd_pts[:,0]]), dx_mat[this_pt[1], this_pt[0]])
        dy_vals=np.append(np.array(dy_mat[nbhd_pts[:,1], nbhd_pts[:,0]]), dy_mat[this_pt[1], this_pt[0]])
        if calc_min_slope is not None:
            dist=np.sqrt((nbhd_pts[:,1]-this_pt[1])**2 + (nbhd_pts[:,0]-this_pt[0])**2)
            dx_slope=np.abs(dx_vals[0:-1]-dx_vals[-1])/dist
            dy_slope=np.abs(dy_vals[0:-1]-dy_vals[-1])/dist
            dxy_slope[row,:]=[np.min(dx_slope), np.min(dy_slope)];

        dxy_range[row,[0,1]]=[np.min(dx_vals), np.max(dx_vals)]
        dxy_range[row,[2,3]]=[np.min(dy_vals), np.max(dy_vals)]
    if calc_min_slope is not None:
        return dxy_range, dxy_slope
    else:
        return dxy_range ##, dxy_bar

def test_epipolar(dxy_0, ep_vec, dxy, tol):
    """
    given an origin vector and an epipolar unit vector, projects a set of
    points against the epipolar vector and gives their perpendicular offset
    WRT that vector, and a boolean array that shows whether this offset is
    smaller that tol.
    """
    delta=np.abs(np.dot((dxy-dxy_0), [ep_vec[1], -ep_vec[0]]))
    disp_mag=np.sqrt((dxy[:,0]-dxy_0[0])**2 +(dxy[:,1]-dxy_0[1])**2)
    good=(delta < tol) | (delta < 0.02 * disp_mag )
    return good, delta

def est_epipolar_vec(dxy, C, C_tol, ep_vec_initial, F_use):
    """
    given a set of offsets and correlation values:
        For those point with  C>C_tol, find the median offset and the
        eigenvectors of the distribution of the points around that median
        offset.  The eigenvector corresponding to the largest eigenvalue is
        assumed to be the epipolar vector.

    """
    if ep_vec_initial is not None:
        return ep_vec_initial, np.zeros([1,2])

    P_use=100*np.array([(1-F_use)/2., (F_use+1.)/2.])
    ctr_vals=(dxy[:,0] >= ss.scoreatpercentile(dxy[:,0], P_use[0])) & (dxy[:,0] <= ss.scoreatpercentile(dxy[:,0], P_use[1])) & (dxy[:,1] >= ss.scoreatpercentile(dxy[:,1], P_use[0])) & (dxy[:,1] <= ss.scoreatpercentile(dxy[:,1], P_use[1]))
    good=ctr_vals & np.array(C.ravel()>C_tol, dtype=bool)
    dxy1=np.array(dxy[good.ravel(),:])
    dxy_ctr=np.array([np.median(dxy1[:,0]), np.median(dxy1[:,1])])
    dxy_0=dxy1.copy()
    dxy_0[:,0]=dxy_0[:,0]-dxy_ctr[0]
    dxy_0[:,1]=dxy_0[:,1]-dxy_ctr[1]
    vals, vecs=np.linalg.eig(np.dot(dxy_0.transpose(), dxy_0))
    biggest_vec=vecs[:,np.argmax(abs(vals)) ]
    return biggest_vec, dxy_ctr

def img_interpolate_linear( x, y, I, GT, nodata_val=np.NaN):
    result=np.zeros_like(x)+nodata_val
    # get the col and row in the image
    col=(x-GT[0])/GT[1]
    row=(y-GT[3])/GT[5]
    good=(row > 0) & (row < I.shape[0]-1) & (col > 0) & (col<I.shape[1]-1)
    if ~np.any(good):
        return result
    # perform interpolation only for in-bounds xy
    row= row[good]
    col= col[good]
    rx=row-np.floor(row)
    ry=col-np.floor(col)
    row= np.floor(row).astype('Int32')
    col= np.floor(col).astype('Int32')
    # add the contributions of each neighbor
    ztemp=np.zeros_like(row)
    ztemp =         (1-ry)*(1-rx)*I[(row, col)]
    ztemp = ztemp + (ry)*(1-rx)*I[(row+1,col)]
    ztemp = ztemp + (1-ry)*(rx)*I[(row,col+1)]
    ztemp = ztemp + (ry)*(rx)*I[(row+1,col+1)]
    result[good]=ztemp
    return result

def grid_disp(xg, yg, xy, dx, dy, ex, ey, L_valid, N_coarse, dispDs, spreadDs, downscale, cr_out):
    grid_spacing=xg[0,1]-xg[0,0]
    # valid kernel dilates the distance mask, which tells whether to correlate
    sigma_valid=L_valid/3;
    N_valid=np.ceil(sigma_valid*3/grid_spacing)
    kernel_valid=np.exp(-0.5*(grid_spacing*np.arange(-N_valid, N_valid+1)/(sigma_valid))**2)
    # smoothing kernel is used to make a large-scale smoothed version of the disparities
    # set its smoothing width to 512 pixels, make sure the whole kernel is at least 1024 pixels wider than the valid kernel
    N_sm=2048/downscale+np.ceil(L_valid/grid_spacing);
    sigma_sm=N_sm/4.
    kernel_sm=np.exp(-0.5*(np.arange(-N_sm, N_sm+1)/sigma_sm)**2)

    row=np.floor((xy[:,1]-yg[0,0])/(yg[1,0]-yg[0,0])).astype('Int32')
    col=np.floor((xy[:,0]-xg[0,0])/(xg[0,1]-xg[0,0])).astype('Int32')
    good=(row>=0) & (row < xg.shape[0]-1) & (col >=0) & (col < xg.shape[1]-1)
    dist_mask=np.zeros_like(xg)
    dist_mask[row[good], col[good]]=1.
    dist_mask=convolve1d(convolve1d(dist_mask, kernel_valid, axis=0, mode='constant'), kernel_valid, axis=1, mode='constant')
    dist_mask=dist_mask > np.exp(-0.5*(L_valid/sigma_valid)**2)

    # take a subset of points from dist_mask to limit the interpolation distance for each quantity
    # Otherwise, the triangulation makes long skinny triangles along concave boundaries of the data points
    mask1=dist_mask[0::N_coarse, 0::N_coarse].ravel()
    xg1=xg[0::N_coarse, 0::N_coarse].ravel()
    yg1=yg[0::N_coarse, 0::N_coarse].ravel()
    xg1=xg1[mask1==0]
    yg1=yg1[mask1==0]
    for count, z in enumerate((dx, dy)):
        zi=griddata(np.append(xy, np.c_[xg1, yg1], 0), np.append(z, np.NaN+np.zeros([len(xg1),1]),0), (xg, yg), method='linear')
        zi_smooth=convolve1d(convolve1d(np.nan_to_num(zi), kernel_sm, axis=0, mode='constant'), kernel_sm, axis=1, mode='constant')
        if count<1:
            mask_smooth=convolve1d(convolve1d((~np.isnan(zi)).astype('float32'), kernel_sm, axis=0, mode='constant'), kernel_sm, axis=1, mode='constant')
        zi_smooth[mask_smooth>1e-5]=zi_smooth[mask_smooth>1e-5]/mask_smooth[mask_smooth>1e-5]
        zi_smooth[mask_smooth<=1e-5]=0
        zi[np.isnan(zi)]=zi_smooth[np.isnan(zi)]
        zi=np.nan_to_num(zi)
        # ... and write it out to the disparity file
        dispDs.GetRasterBand(count+1).WriteArray(zi[:,:,0], int(cr_out[0]), int(cr_out[1]))
        if count==0:
            dx_g=zi
        if count==1:
            dy_g=zi
    dispDs.GetRasterBand(3).WriteArray(dist_mask.astype('float32'), int(cr_out[0]), int(cr_out[1]))
    zi=np.nan_to_num(griddata(np.append(xy, np.c_[xg1, yg1], 0), np.append(ex, np.NaN+np.zeros([len(xg1),1]),0), (xg, yg), method='linear'))
    zi[dist_mask==0]=0
    spreadDs.GetRasterBand(1).WriteArray(zi[:,:,0], int(cr_out[0]), int(cr_out[1]))
    zi=np.nan_to_num(griddata(np.append(xy, np.c_[xg1, yg1], 0), np.append(ey, np.NaN+np.zeros([len(xg1),1]),0), (xg, yg), method='linear'))
    zi[dist_mask==0]=0
    spreadDs.GetRasterBand(2).WriteArray(zi[:,:,0], int(cr_out[0]), int(cr_out[1]))
    spreadDs.GetRasterBand(3).WriteArray(dist_mask.astype('float32'), int(cr_out[0]), int(cr_out[1]))
    return

def main():
    """ For a pair of images, perform a sparse image match using the full
    resolution of the images.  Begin at resolution coarse_skip.  For each
    point in a grid spanning the image 1, extract a template image of
    size t_pixels, and use fft-based correlation to find its match in image 2,
    over a wide search window (size xsearch by ysearch).  Based on matches from
    this coarse grid, estimate an epipolar vector (the largest eigenvector of
    the disparity vectors) and a tolerance for disparities to be parallel to
    this vector (use max (16 pix, 90th percentile of differeces)).

    Next, refine points for which the range of disparities among the
    neighbors are larger than tolerance refine_tol.  Refinement consists
    of adding the eight grid neighbors of a point, at half the previous
    point spacing.  During refinment, the search windows are selected to span
    the neighbors' x and y disparities, and are padded by 16 pixels and padded
    again to give a power-of-two search size.

    After each refinement, the new matches are checked againt the epipolar
    vector, and all the matched points are checked for convergence.  Refinment
    continues until the point spacing reaches fine_skip, or until no points
    need to be refined.

    Results are written to oufile:
        pixel_x
        pixel_y
        x_disparity
        y_diparity
        correlation value
        (max-min) neighbor x disparity
        (max-min) neighbor y disparity
     """

    usage = '''%prog [options] left_image right_image output_prefix

  [ASP [@]ASP_VERSION[@]]'''

    parser=OptionParser(usage=usage)
    parser.add_option("-x", "--xsearch", dest="s_nx", default=1024-56, type="int",
                      help="initial x search range, pixels (%default)")
    parser.add_option("-y", "--ysearch", dest="s_ny", default=1024-56, type="int",
                      help="initial y search range, pixels (%default)")
    parser.add_option("-t", "--template", dest="t_pixels", default=56, type="int",
                      help="template size, pixels (%default)")
    parser.add_option("-c", "--coarse", dest="coarse_skip", default=None, type="int",
                      help="initial search-point spacing, pixels (%default)")
    parser.add_option("-f", "--fine", dest="fine_skip", default=64, type="int",
                      help="final search-point spacing, pixels (%default)")
    parser.add_option("-r","--refine_tol", dest="refine_tol", default=8, type="int",
                      help="pixel range tolerance for refinement (%default)")
    parser.add_option("-d","--output_dec_scale", dest="output_scale",  default=8, type="int",
                      help="if set, output a second copy of the offsets scaled by this amount (%default), to file called OUTFILE_(OUTPUT_SCALE)")
    parser.add_option("-p","--output_pad",dest="output_pad", default=2, type="int",
                      help="pad the output search range by this amount (%default)")
    parser.add_option("-s", "--sigma_t_min", dest="sigma_t_min",  default=0., type="float",
                       help="matches will not be calculated if the standard deviation of the template is less than this value (%default)")
    parser.add_option("-R", "--R_lim_min", dest="R_lim_min",  default=32., type="float",
                       help="The minimum-neighbor-disparity parameter is set to the maximum of this parameter and the 98th percentile of all minimum neighbor disparities.\n  Points whose neighbors have a minimum disparity range larger than this parameter will be removed (%default)")
    parser.add_option("-l", "--R_lim_max", dest="R_lim_max",  default=128., type="float",
                       help="The minimum-neighbor-disparity parameter is set to the minimum of this parameter and the 98th percentile of all minimum neighbor disparities.\n  Points whose neighbors have a minimum disparity range larger than this parameter will be removed (%default)")
    parser.add_option("-m", "--mask_file", dest="mask_file", default=None, type="string",
                      help="if specified, use this geotif mask file to reject template-image points that should not be searched (1=search, 0=don't)")
    parser.add_option("-M", "--Max_disp_range", dest="M_max",  default=64., type="float",
                       help="Un-scaled disparity range in output limited to this value (%default)")
    parser.add_option("-P", "--processes", dest="processes", default=cpu_count(), type="int",
                      help="The number of processes to use (%default)")
    parser.add_option("-e", "--epipolar_axis", dest="epipolar_axis", default=None, type="int",
                      help="If set, specify the axis that is epipolar 0=x, 1=y (%default)")
    parser.add_option("-n", "--nodata-value", dest="user_nodata", default=None, type="int",
                      help="The no-data value (pixel values <= nodata are not not used. (%default)")
    parser.add_option("-w", "--fill-dist", dest="fill_dist", default=1000., type="float",
                      help="Fill in gaps of this size or more with smoothed values. (%default)")
    parser.add_option("-D", "--Debug", dest="Debug", default=False, action="store_true",
                      help="Output deugging info and text file of correlation-estimate points")
    (options, args) = parser.parse_args()

    if sys.version_info < (2, 6):
        die('\nERROR: Must use Python 2.6 or greater.', code=2)

    if len(args) < 3:
        parser.print_help()
        die('\nERROR: Missing input files or output prefix', code=2)

    if options.mask_file is not None:
        mask_file=gdal.Open(options.mask_file)
        in_mask={'GT': np.array(mask_file.GetGeoTransform()), 'Z': np.array(mask_file.ReadAsArray())}
    else:
        in_mask=None

    #undocumented options#
    F_ep_pts_to_use=0.9
    ep_tol_max=24
    ep_tol_min=4
    dxy_slope_tol=3.0
    C_tol=0.4  # minimum quality needed in defining the epipolar vector estimate

    T_file = args[0]
    S_file = args[1]
    output_prefix = args[2]
    sys.stdout.flush()
    print "Start: " + str(datetime.datetime.now())
    print "T_file = " + T_file
    print "S_file = " + S_file
    print "output_prefix = " + output_prefix

    #Minimum number of coarse points in any dimension
    min_npts = 8

    # Adjust the input parameters based on the images. This
    # may need to become more advanced.
    T_ds=gdal.Open(T_file, gdalconst.GA_ReadOnly)
    T_band=T_ds.GetRasterBand(1)
    prev2x = prevpow2( T_band.XSize)
    if options.s_nx > prev2x/4:
        options.s_nx = prev2x/4
        print "Warning: Decreasing xsearch value to: %d" % options.s_nx

    prev2y = prevpow2(T_band.YSize)
    if options.s_ny > prev2y/4:
        options.s_ny = prev2y/4
        print "Warning: Decreasing ysearch value to: %d" % options.s_ny

    #Automatically set coarse_skip if not specified
    if options.coarse_skip is None:
        options.coarse_skip = np.min([prev2x/min_npts, prev2y/min_npts])
        print "Setting coarse skip to: %d" % options.coarse_skip
    else:
        #Check to make sure coarse_skip isn't TOO coarse
        #Minimum of 4 points in smallest dimension
        if options.coarse_skip > np.min([prev2x/min_npts, prev2y/min_npts]):
            options.coarse_skip = np.min([prev2x/min_npts, prev2y/min_npts])
            print "Warning: decreasing coarse skip to: %d" % options.coarse_skip

    s_nx=options.s_nx
    s_ny=options.s_ny
    t_pixels=options.t_pixels

    if options.epipolar_axis is not None:
        ep_vec_initial=np.zeros([1,2])
        ep_vec_initial[options.epipolar_axis]=1.
        if options.epipolar_axis==0:
            s_ny=np.min([s_ny, s_nx/10.]);
        else:
            s_nx=np.min([s_nx, s_ny/10.]);
    else:
        ep_vec_initial=None

    out_dir=os.path.dirname(output_prefix)
    if len(out_dir)==0: out_dir='.'
    if not os.path.exists(out_dir): os.mkdir(out_dir)

    skip_vals=2.**np.arange(np.floor(np.log2(options.coarse_skip/2.)),
                            np.floor(np.log2(options.fine_skip/2.)), -1)

    #initialize the matcher object
    ftm=fft_matcher(T_file, S_file, options.processes, options.user_nodata)

    # define the initial search points
    edge_pad=np.array([s_nx/4.+t_pixels/2, s_ny/4.+t_pixels/2])
    XC=np.arange(ftm.T_c0c1[0]+edge_pad[0], ftm.T_c0c1[1]-edge_pad[0], options.coarse_skip)
    if XC[-1] < ftm.T_c0c1[1]:
        XC=np.append(XC, int((XC[-1]+ftm.T_c0c1[1])/2.))
    YC=np.arange(ftm.T_r0r1[0]+edge_pad[1], ftm.T_r0r1[1]-edge_pad[1], options.coarse_skip)
    if YC[-1] < ftm.T_r0r1[1]:
        YC=np.append(YC, int((YC[-1]+ftm.T_r0r1[1])/2.))
    [XCg, YCg]=np.meshgrid(XC, YC)
    XY0=np.c_[XCg.ravel(), YCg.ravel()]

    # find the offset that matches the origins of the two images
    GT=ftm.GT_S
    D_origin = np.c_[ftm.UL_T-ftm.UL_S].transpose()
    D_origin[0][0]=np.floor(D_origin[0][0]/np.abs(GT[1]))
    D_origin[0][1]=np.floor(D_origin[0][1]/np.abs(GT[5]))
    print "Running initial search: " + str(datetime.datetime.now())

    dxy0=np.dot(np.c_[np.ones_like(XY0[:,0])], D_origin*[1, -1])
    dxy_score=np.c_[dxy0[:,0]-s_nx/2., dxy0[:,0]+s_nx/2, dxy0[:,1]-s_ny/2, dxy0[:,1]+s_ny/2]
    xy, dxy, C, xy_bad_mask = search_new_pts(XY0, dxy_score, t_pixels, ftm, min_template_sigma=options.sigma_t_min, mask=in_mask)

    # run the epipolar fit, establish a tolerance, run again
    ep_vec, dxy_ctr=est_epipolar_vec(dxy, C, C_tol, ep_vec_initial, F_ep_pts_to_use)
    good, ep_dist=test_epipolar(dxy_ctr, ep_vec, dxy, 32)
    ep_f90=ss.scoreatpercentile(ep_dist[C.ravel()>C_tol], 90)
    ep_tol=np.minimum(ep_tol_max,np.maximum(ep_tol_min, ep_f90))
    # run fit again if any points were marked as bad in the first epipolar test
    if ep_vec_initial is not None and np.any(~good):
        ep_vec, dxy_ctr=est_epipolar_vec(dxy[good,:], C[good,:], C_tol, None, F_ep_pts_to_use)
        ep_f90= ss.scoreatpercentile(ep_dist[C.ravel()>C_tol], 90)
        ep_tol=np.minimum(ep_tol_max,np.maximum(ep_tol_min, ep_f90))
        good, ep_dist=test_epipolar(dxy_ctr, ep_vec, dxy, ep_tol)
    print " --- ep vec estimated at(%f,%f), tol=%f, ep_dist_f90=%f" % (ep_vec[0], ep_vec[1], ep_tol, ep_f90)
    # make sparse matrices for storing dx and dy values, store initial values
    im_shape=[ftm.Ny, ftm.Nx]
    dx_mat=coo_matrix((dxy[good,0],(xy[good,1],xy[good,0])), shape=im_shape).tocsr()
    dy_mat=coo_matrix((dxy[good,1],(xy[good,1],xy[good,0])), shape=im_shape).tocsr()
    C_mat=coo_matrix(((C[good]).ravel(), (xy[good,1], xy[good,0])), shape=im_shape).tocsr()
    bad_mask_mat = coo_matrix((np.ones_like(xy_bad_mask[:,0]), (xy_bad_mask[:,1], xy_bad_mask[:,0])), shape=im_shape).tocsr()
    xy_list=xy[good,:]
    # points tested so far are the nozero members of C_mat
    all_pts=np.c_[C_mat.nonzero()];
    all_pts=all_pts[:,[1,0]];
    tri=sp.Delaunay(all_pts)
    pt_dict=make_pt_2_neighbors(tri)

    dxy_score=nbhd_range(np.arange(0, all_pts.shape[0]), dx_mat, dy_mat, tri, pt_dict)
    refine_pts=np.arange(0, xy_list.shape[0] )
    # define the pattern of pixel centers to refineat each steps.  Duplicates
    # will be deleted.
    refine_x=np.array([-1.,  0.,  1., -1.,  1., -1., 0., 1.])
    refine_y=np.array([-1., -1., -1.,  0.,  0.,  1., 1., 1.]);
    recalc_nbhd_range=False
    for delta_x in skip_vals:
        print "----------refining to %d---------" % delta_x
        print "Refining: " + str(datetime.datetime.now())
        if len(refine_pts)==0:
            print "    No refinement points for scale: %d" % delta_x
            recalc_nbhd_range=True
            break
        # add neighbors of the last set of points to the list
        #sc=plt.scatter(all_pts[refine_pts,0], all_pts[refine_pts,1], c=dxy_score[:,1]-dxy_score[:,0]); plt.axis('equal'); plt.colorbar(sc)
        new_x=(np.tile(all_pts[refine_pts, 0], [8,1]).transpose()+refine_x*delta_x).ravel()
        new_y=(np.tile(all_pts[refine_pts, 1], [8,1]).transpose()+refine_y*delta_x).ravel()
        # the search range for the new points is set based on the min and max of the dxy_scores of the refined points
        new_dxy_score=np.array([np.tile(dxy_score[:,0], [8,1]).transpose().ravel(),
                       np.tile(dxy_score[:,1], [8,1]).transpose().ravel(),
                       np.tile(dxy_score[:,2], [8,1]).transpose().ravel(),
                       np.tile(dxy_score[:,3], [8,1]).transpose().ravel()]).transpose()

        # define min and max pt indices
        x_lims=np.array([1, im_shape[1]-1])
        y_lims=np.array([1, im_shape[0]-1])
        # clamp range of x and y to these indices
        new_x[new_x > x_lims[1]]=x_lims[1]
        new_x[new_x < 0 ]=x_lims[0]
        new_y[new_y > y_lims[1]]=y_lims[1]
        new_y[new_y < 0 ]=y_lims[0]

        not_dups=np.squeeze(np.array(C_mat[new_y, new_x]==0))
        new_xy=np.c_[new_x[not_dups], new_y[not_dups]]
        uRows, new_xy=unique_rows(new_xy)
        new_dxy_score=new_dxy_score[uRows,:]
        N_search=new_xy.shape[0]

        new_xy, new_dxy,  new_C, new_xy_bad=search_new_pts(new_xy, new_dxy_score, t_pixels, ftm, min_template_sigma=options.sigma_t_min, mask=in_mask)
        if len(new_xy)==0:
            print "    no new points found"
            recalc_nbhd_range=True
            break

        good, ep_dist=test_epipolar(dxy_ctr, ep_vec, new_dxy, ep_tol)
        print "    searched %d points, found %d good matches" % (N_search, np.sum(good) )
        # add the new points to the sparse matrix of tested points
        dx_mat=dx_mat+coo_matrix((new_dxy[good,0], [new_xy[good,1], new_xy[good,0]]), im_shape).tocsr()
        dy_mat=dy_mat+coo_matrix((new_dxy[good,1], [new_xy[good,1], new_xy[good,0]]), im_shape).tocsr()
        C_mat=C_mat+coo_matrix(((new_C[good]).ravel(), [new_xy[good,1], new_xy[good,0]]), im_shape).tocsr()
        bad_mask_mat = bad_mask_mat + coo_matrix((1*np.ones_like(new_xy_bad[:,0]), (new_xy_bad[:,1], new_xy_bad[:,0])), shape=im_shape).tocsr()
        bad=~good
        bad_mask_mat = bad_mask_mat + coo_matrix((2*np.ones_like(new_xy[bad,1]), (new_xy[bad,1], new_xy[bad,0])), shape=im_shape).tocsr()

        #ID_new_pts=lil_matrix((np.ones_like(new_dxy[good,0]), [new_xy[good,1], new_xy[good,0]]), im_shape).tocsr()

        all_pts=np.c_[C_mat.nonzero()];
        all_pts=all_pts[:,[1,0]];
        # extract the dx and dy values, re-estimate the ep vector
        dxy=np.array(np.c_[dx_mat[all_pts[:,1], all_pts[:,0]].transpose(), dy_mat[all_pts[:,1], all_pts[:,0]].transpose()])
        C= np.array(C_mat[all_pts[:,1], all_pts[:,0]].transpose())
        if ep_vec_initial is not None:
            ep_vec, dxy_ctr=est_epipolar_vec(dxy, C, C_tol)

        # triangulate all the good points so far
        tri=sp.Delaunay(all_pts)
        pt_dict=make_pt_2_neighbors(tri)
        # zero out points for which delta(disparity)/delta(dist) is too large
        dxy_score, min_dxy_slope=nbhd_range(range(all_pts.shape[0]), dx_mat, dy_mat, tri, pt_dict, calc_min_slope=True)
        bad=np.max(min_dxy_slope, axis=1) > dxy_slope_tol
        if options.Debug:
            print "---deleting %d points that failed the d(disparity)/d(dist) test" % np.sum(bad)
        if bad is not None and np.any(bad):
            C_mat=C_mat.tolil()
            C_mat[all_pts[bad,1], all_pts[bad,0]]=0
            C_mat=C_mat.tocsr()
            bad_mask_mat = bad_mask_mat + coo_matrix((4*np.ones_like(all_pts[bad,1]), (all_pts[bad,1], all_pts[bad,0])), shape=im_shape).tocsr()

            all_pts=np.c_[C_mat.nonzero()]
            all_pts=all_pts[:,[1,0]];
            tri=sp.Delaunay(all_pts)
            pt_dict=make_pt_2_neighbors(tri)
            dxy_score = nbhd_range(range(all_pts.shape[0]), dx_mat, dy_mat, tri, pt_dict)


        ## don't refine if we're on the last value of the refinement list
        if delta_x == skip_vals[-1]:
            continue
        test_pts=np.arange(0, all_pts.shape[0])
        # test the new points and their neighbors for convergence
        to_refine=np.logical_or((dxy_score[:,1]-dxy_score[:,0]>options.refine_tol), (dxy_score[:,3]-dxy_score[:,2]) > options.refine_tol)
        refine_pts=test_pts[to_refine,:]
        dxy_score=dxy_score[to_refine,:]
        # N.B.  we can often end up refining more points than we searched on the
        # last round, because points from previous rounds can get marked for
        # refinement
        print "    found %d points to refine" %  len(refine_pts)

    if recalc_nbhd_range:
        all_pts=np.c_[C_mat.nonzero()];
        all_pts=all_pts[:,[1,0]];
        dxy_score = nbhd_range(range(all_pts.shape[0]), dx_mat, dy_mat, tri, pt_dict)

    # delete the 1% of matches with the greatest disparity range
    R_dx=dxy_score[:,1]-dxy_score[:,0]
    R_dy=dxy_score[:,3]-dxy_score[:,2]

    R_dx_mat=coo_matrix((R_dx, [all_pts[:,1], all_pts[:,0]]), im_shape).tocsr()
    R_dy_mat=coo_matrix((R_dy, [all_pts[:,1], all_pts[:,0]]), im_shape).tocsr()
    R_dxy_score=nbhd_range(range(all_pts.shape[0]), R_dx_mat, R_dy_mat, tri, pt_dict)
    P99=(ss.scoreatpercentile(R_dxy_score[:,0], 99), ss.scoreatpercentile(R_dxy_score[:,2], 99))
    R_max = np.maximum(P99, options.R_lim_min)
    R_max = np.minimum(R_max, options.R_lim_max)
    bad_xy=all_pts[np.logical_or((R_dxy_score[:,0] > R_max[0]) ,  (R_dxy_score[:,2] > R_max[1])),:]
    bad_mask_mat = bad_mask_mat + coo_matrix((8*np.ones_like(bad_xy[:,1]), (bad_xy[:,1], bad_xy[:,0])), shape=im_shape).tocsr()
    if options.Debug:
        print "rejecting %d detected outliers using the minimum-disparity-difference test with R_max = %f, %f" % (bad_xy.shape[0], R_max[0], R_max[1])
    # delete the bad value, convert dx dy, and C to lil matrices
    C_mat=C_mat.tolil()
    dx_mat=dx_mat.tolil()
    dy_mat=dy_mat.tolil()
    for bad in bad_xy:
        dx_mat[bad[1], bad[0]]=0
        dy_mat[bad[1], bad[0]]=0
        C_mat[bad[1], bad[0]]=0
    C_mat=C_mat.tocsr()
    dx_mat=dx_mat.tocsr()
    dy_mat=dy_mat.tocsr()
    all_pts=np.c_[C_mat.nonzero()];
    all_pts=all_pts[:,[1,0]];
    # triangulate all the good points so far
    tri=sp.Delaunay(all_pts)
    pt_dict=make_pt_2_neighbors(tri)

    # check the score for output, ignore differences for points separated by more than 2*coarse skip
    dxy_score=nbhd_range(range(all_pts.shape[0]), dx_mat, dy_mat, tri, pt_dict, max_dist= 2.*options.coarse_skip)
    R_dx=dxy_score[:,1]-dxy_score[:,0]
    R_dy=dxy_score[:,3]-dxy_score[:,2]

    print "--------output stats------"
    for pct in (99, 95, 90, 84):
        P= (pct, ss.scoreatpercentile(R_dx, pct), ss.scoreatpercentile(R_dy, pct))
        print "%dth percetiles (rx,ry) = (%5.2f %5.2f)" % P

    dx=dx_mat[all_pts[:,1], all_pts[:,0]].transpose()
    dy=dy_mat[all_pts[:,1], all_pts[:,0]].transpose()
    C=C_mat[all_pts[:,1], all_pts[:,0]].transpose()

    # transform the pixel centers to map coordinates
    GT = ftm.GT_S
    driver = ftm.T_ds.GetDriver()
    xy=np.c_[GT[0]+all_pts[:,0]*GT[1]+all_pts[:,1]*GT[2], GT[3]+all_pts[:,0]*GT[4]+all_pts[:,1]*GT[5]]
    out=np.c_[xy, dx , dy , C, dxy_score[:,1]-dxy_score[:,0]+options.output_pad, dxy_score[:,3]-dxy_score[:,2]+options.output_pad]
#    # spit out the good and bad masks
    good_xy=np.c_[C_mat.nonzero()][:,[1, 0]]
    good_xy=np.c_[GT[0]+good_xy[:,0]*GT[1]+good_xy[:,1]*GT[2], GT[3]+good_xy[:,0]*GT[4]+good_xy[:,1]*GT[5]]
    bad_xy=np.c_[bad_mask_mat.nonzero()][:,[1, 0]]
    if np.min(bad_xy.shape)==0:
        bad_flag=bad_mask_mat[bad_xy[:,1], bad_xy[:,0]]
        bad_xy=np.c_[GT[0]+bad_xy[:,0]*GT[1]+bad_xy[:,1]*GT[2], GT[3]+bad_xy[:,0]*GT[4]+bad_xy[:,1]*GT[5]]
    else:
        bad_flag=None
        bad_xy=None
    # cleanup memory before gridding starts
    pt_dict=None
    tri=None
    dx=None
    dy=None
    dx_mat=None
    dy_mat=None
    R_dx_mat=None
    R_dy_mat=None
    xy=None
    ftm=None
    C=None
    refine_pts=None


    if options.Debug:
        print "--- Debug enabled-----"
        print " writing csv file: %s" % output_prefix+'.csv'
        outfile=open(output_prefix+'.csv','w')
        outfile.write('%x, y, dx, dy, C, R_dx, R_dy\n')
        for line in out:
            outfile.write("%7.0f, %7.0f, %7.0f, %7.0f, %4.2f, %7.0f, %7.0f\n" %  tuple(x for x in line.tolist()[0]))
        outfile.close
        print " writing bad points to file: %s" % output_prefix+'_bad_pts.csv'
        print " bad matches are flagged:"
        print "  1: failed signal strength test"
        print "  2: failed epipolar test"
        print "  4: failed d(disparity)/d(dist) test"
        print "  8: failed the total-delta-disparity test"
        print "-----"
        outfile=open(output_prefix+'_bad_pts.csv', 'w')
        outfile.write('%x, y, flag\n')
        if bad_flag is not None:
            out_bad=np.c_[bad_xy[:,0], bad_xy[:,1], bad_flag.transpose()]
            for line in out_bad:
                outfile.write("%7.2f, %7.2f, %3d\n" %  tuple(x for x in line.tolist()[0]))
        outfile.close
        out_bad=None
        bad_flag=None

    if options.output_scale > 0.:
        out=out/options.output_scale
        # scale x, y, and C back up by output_scale
        for col in [0, 1, 4]:
            out[:,col]=out[:,col]*options.output_scale
        delta_x=options.output_scale*np.abs(GT[1])
        x0=GT[0]
        y0=GT[3]
        x0=delta_x*(np.ceil(x0/delta_x))
        y0=delta_x*(np.ceil(y0/delta_x))
        Gdx=GT[1]*options.output_scale
        Gdy=GT[5]*options.output_scale

        GT1=np.array(GT)
        GT1[0]=x0;
        GT1[3]=y0;
        GT1[1]=Gdx
        GT1[5]=Gdy
        nx1=np.int(im_shape[1]/options.output_scale)
        ny1=np.int(im_shape[0]/options.output_scale)
        xg=(np.arange(0, nx1)+0.5)*Gdx+x0
        yg=(np.arange(0, ny1)+0.5)*Gdy+y0
        disp_file = output_prefix + '-D_sub.tif'
        dispDs = driver.Create(disp_file,  nx1, ny1, 3, gdalconst.GDT_Int32)
        spread_file = output_prefix + '-D_sub_spread.tif'
        spreadDs = driver.Create(spread_file,  nx1, ny1, 3, gdalconst.GDT_Int32)
        # interpolate the scaled dx and dy values
        OutBlocksize=1024
        c0_out=np.arange(0,  xg.shape[0], OutBlocksize)
        r0_out=np.arange(0,  yg.shape[0], OutBlocksize)
        grid_pad=options.fill_dist*2
        for c0 in c0_out:
            for r0 in r0_out:
                if options.Debug:
                    print "     gridding output for col %d out of %d, row %d out of %d" % (int(c0/OutBlocksize), int(c0_out[-1]/OutBlocksize), int(r0/OutBlocksize), int(r0_out[-1]/OutBlocksize))
                cols=np.arange(c0, np.minimum(c0+OutBlocksize, len(xg)))
                rows=np.arange(r0, np.minimum(r0+OutBlocksize, len(yg)))
                if (len(rows)==0) | (len(cols)==0):
                    continue
                [xg_sub, yg_sub]=np.meshgrid(xg[cols], yg[rows])
                cr_out=[cols[0], rows[0]]
                ii=np.zeros(out.shape[0]).astype('bool')  # make a 1-d array to populate with the truth values we'll use to select data points
                ii[:]=((out[:,0] > xg_sub[0,0]-grid_pad) & (out[:,0] < xg_sub[0,-1]+grid_pad) & (out[:,1] > yg_sub[-1,0]-grid_pad) & (out[:,1] < yg_sub[0,0]+grid_pad)).ravel()
                grid_disp(xg_sub, yg_sub, out[ii,0:2], out[ii,2], out[ii,3], out[ii,5], out[ii,6], options.fill_dist, s_nx, dispDs, spreadDs, options.output_scale, cr_out)

        for Ds in (dispDs, spreadDs):
            Ds.SetGeoTransform(tuple(GT1))
            Ds.SetProjection(ftm.T_ds.GetProjection())
        spreadDs=None
        dispDs=None

    print "End: " + str(datetime.datetime.now())

if __name__=="__main__":
    main()
